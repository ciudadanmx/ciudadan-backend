const axios = require("axios");

const LM_STUDIO_URL = process.env.LM_STUDIO_URL || "http://192.168.1.3:1234/v1/chat/completions";
const MODEL_NAME = process.env.MODEL_NAME || "deepseek-r1-distill-qwen-7b";

async function getLlmResponse(messages, res) {
  try {
    const response = await axios.post(
      LM_STUDIO_URL,
      {
        model: MODEL_NAME,
        messages,
        temperature: 0.7,
        max_tokens: 500,
        stream: true, // ‚úÖ Activar streaming
      },
      { responseType: "stream" }
    );

    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    let buffer = ""; // ‚úÖ Almacena la respuesta parcial

    response.data.on("data", (chunk) => {
      const data = chunk.toString().trim();
      console.log("üì• Chunk recibido:", data);

      if (data === "[DONE]") {
        console.log("‚úÖ Finalizando streaming...");
        res.write("\n\n");
        res.end(); // ‚úÖ Cerrar correctamente
        return;
      }

      try {
        const parsed = JSON.parse(data.replace(/^data: /, ""));
        const text = parsed.choices?.[0]?.delta?.content || "";

        buffer += text; // ‚úÖ Acumula la respuesta
        res.write(text);
      } catch (error) {
        console.error("‚ùå Error parseando chunk:", error);
      }
    });

    response.data.on("end", () => {
      console.log("‚úÖ Respuesta final:", buffer);
      res.end();
    });

  } catch (error) {
    console.error("‚ùå Error al obtener respuesta del LLM:", error?.response?.data || error.message);
    res.status(500).json({ error: "Error en la generaci√≥n de respuesta." });
  }
}

module.exports = { getLlmResponse };
